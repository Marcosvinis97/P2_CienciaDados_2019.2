{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <font color=red> Engenharia 2019.2 - Ci√™ncia dos dados </font> </h1>\n",
    "\n",
    "<h2> Projeto 2 - Classificador Autom√°tico de Sentimento</h2>\n",
    "\n",
    "<h3> Alunos: </h3>\n",
    "<ul>\n",
    "    <li>Cicero Tiago Carneiro Valentim</li>\n",
    "    <li>Luiz Felipe Lazzaron</li>\n",
    "    <li>Marcos Vin√≠cius</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Sum√°rio: </h3>\n",
    "<ol>\n",
    "   <li> Introdu√ß√£o </li>\n",
    "   <p></p>\n",
    "   <li>Preparando Ambiente no Jupyter</li>\n",
    "   <p></p>\n",
    "   <li>Autenticando no Twitter</li>\n",
    "   <p></p>\n",
    "   <li>Escolha de um produto</li>\n",
    "   <p></p>\n",
    "   <li>Classificando as mensagens na coragem</li>\n",
    "   <p></p>\n",
    "   <li>Montando o Classificador Naive-Bayes</li>\n",
    "   <p></p>\n",
    "   <li>Limpeza e Tratamento de Dados</li>\n",
    "   <ul>\n",
    "       <li>7.1 Fun√ß√£o Remove;</li>\n",
    "       <li>7.2 Fun√ß√£o SpaceCleaning;</li>\n",
    "       <li>7.3 Fun√ß√£o TratamentodoGrupo;</li>\n",
    "       <li>7.4 Fun√ß√£o Stemming;</li>\n",
    "       <li>7.5 Fun√ß√£o RemoveStopWords;</li>\n",
    "       <li>7.6 Tratando os Dados da Base de Treinamento;</li>\n",
    "   </ul>\n",
    "   <p></p>\n",
    "   <li>Naive Bayes e Laplace Smoothing</li>\n",
    "   <ul>\n",
    "       <li>8.1 Fun√ß√£o lista_sem_repeti√ß√£o</li>\n",
    "       <li>8.2 Fun√ß√£o laplace</li>\n",
    "       <li>8.3 Fun√ß√£o numero_de_vezes</li>\n",
    "       <li>8.4 Fun√ß√£o conta_palavras</li>\n",
    "   </ul>\n",
    "   <p></p>\n",
    "   <li>Verificando a performance na Planilha Testes</li>\n",
    "   <ul>\n",
    "       <li>9.1 Fun√ß√£o limpeza_tweet;</li>\n",
    "       <li>9.2 Fun√ß√£o analisa_frase;</li>\n",
    "       <li>9.3 Fun√ß√£o identifica_valor_maximo;</li>\n",
    "       <li>9.4 Fun√ß√£o resultado;</li>       \n",
    "   </ul>\n",
    "   <p></p>\n",
    "   <li>Conclus√£o</li>\n",
    "   <p></p>\n",
    "   <li>Refer√™ncias</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introdu√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    No presente trabalho, visa-se selecionar tweets relevantes para a classifica√ß√£o de postagens que possuem sinais de depress√£o por parte de seus autores. Nesse contexto, os integrantes dividiram os tweets em tr√™s categorias:\n",
    "</p>\n",
    "<ul>\n",
    "    <li> 1. Irrelavantes ou Neutros. Tweets que n√£o indicam quaisquer ind√≠cio de depress√£o;</li>\n",
    "    <li> 2. Relevantes. Tweets que possuem relev√¢ncia na an√°lise de indica√ß√£o de depress√£o;</li>\n",
    "    <li> 3. Possuem interesse no assunto. Tweets que n√£o demonstram doen√ßa mental, mas indicam um interresse no tema e na preven√ß√£o de suic√≠dio;</li>\n",
    "</ul>\n",
    "<p>\n",
    "    Desse modo, com tal classifica√ß√£o, pode-se desenvolver um marketing espec√≠fico para os p√∫blicos 2 e 3; para os relevantes, pode-se oferecer aux√≠lio e suporte psicol√≥gico; para aqueles que possuem interesse no tema \"depress√£o\", pode-se direcionar materiais que ensinem a como lidar com pessoas com depress√£o.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\user\\anaconda3\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "!pip install nltk\n",
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import script\n",
    "from emoji import UNICODE_EMOJI\n",
    "import emoji\n",
    "import re \n",
    "import functools\n",
    "import operator\n",
    "import nltk \n",
    "from nltk.stem import RSLPStemmer\n",
    "nltk.download('rslp')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autenticando no  Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conta: ***@MarcosV96118169***\n",
    "\n",
    "#Dados de autentica√ß√£o do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @MarcosV96118169\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. N√£o modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Escolha de um produto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "Produto escolhido: <b>Depress√£o</b>\n",
    "\n",
    "Quantidade m√≠nima de mensagens capturadas: <b>750</b>\n",
    "\n",
    "Quantidade m√≠nima de mensagens para a base de treinamento: <b>300</b>\n",
    "\n",
    "Filtro de l√≠ngua: <b>pt</b>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classificando as mensagens na coragem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta etapa foi manual e foi feita diretamente no Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lendo arquivos xlsx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "planilha_treinamento = pd.read_excel('depressao.xlsx', \"Treinamento\")\n",
    "planilha_teste = pd.read_excel('depressao.xlsx', \"Teste\")\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Montando o Classificador Naive-Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Primeiramente, iremos trabalhar com o <i>dataframe</i> treinamento, criando tr√™s banco de dados:</p>\n",
    "<ul>\n",
    "    <li>Grupo 1 - Irrelevantes/Neutros: group_1</li>\n",
    "    <li>Grupo 2 - Relevantes: group_2</li>\n",
    "    <li>Grupo 3 - Interesse no Assunto: group_3</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = planilha_treinamento[planilha_treinamento[\"Relev√¢ncia\"] == 1]\n",
    "group_2 = planilha_treinamento[planilha_treinamento[\"Relev√¢ncia\"] == 2]\n",
    "group_3 = planilha_treinamento[planilha_treinamento[\"Relev√¢ncia\"] == 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Limpeza e Tratamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Fun√ß√£o Remove:\n",
    "<p>Essa fun√ß√£o serve para retirarmos do nosso <i>DataFrame</i> qualquer string com as iniciais <b>http</b> e <b>@.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove(dataframe):\n",
    "    data_aux = dataframe\n",
    "    lista_tweets = list(dataframe.loc[:,\"Tweet Text\"].values)\n",
    "    lista_reserva = []\n",
    "    for i in range(len(lista_tweets)):\n",
    "        lista_tweets[i] = lista_tweets[i].split()\n",
    "        for t in range(len(lista_tweets[i])):\n",
    "            if \"http\" in lista_tweets[i][t]:\n",
    "                lista_tweets[i][t] = \"http\"\n",
    "        while lista_tweets[i].count(\"http\") != 0:\n",
    "            lista_tweets[i].remove(\"http\")\n",
    "        lista_tweets[i] = ' '.join(lista_tweets[i])\n",
    "    data_aux.loc[:,\"Tweet Text\"] = lista_tweets\n",
    "    return data_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Fun√ß√£o SpaceCleaning:\n",
    "<p>Essa fun√ß√£o serve para quebrarmos nosso texto por palavras e emojis, criando desse modo um array com todas as palavras e mojis contidas dentro do texto. Al√©m disso, retiramos do nosso array algumas pontua√ß√µes desnecess√°rias para a an√°lise.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpaceCleaning(text):\n",
    "    punctuation = '[!\\-.:?;,''\"@/]' \n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    split_emoji = emoji.get_emoji_regexp().split(text_subbed)\n",
    "    split_whitespace = [substr.split() for substr in split_emoji]\n",
    "    split = functools.reduce(operator.concat,split_whitespace)\n",
    "    return split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Fun√ß√£o TratamentodoGrupo:\n",
    "<p>Essa fun√ß√£o serve para tratar a base de dados do grupo 1, 2 e 3.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TratamentodoGrupo(grupo):\n",
    "    total = []\n",
    "    for tweet in grupo[\"Tweet Text\"]:\n",
    "        tweet_splited = SpaceCleaning(tweet)\n",
    "        total.append(tweet_splited)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Fun√ß√£o Stemming:\n",
    "<p>Essa fun√ß√£o serve para diminuirmos a palavra at√© a sua raiz, pois assim, conseguimos tratar as palavras originais e suas respectivas deriva√ß√µes de uma mesma maneira.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stemming(sentence):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        phrase.append(stemmer.stem(word.lower()))\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Fun√ß√£o RemoveStopWords:\n",
    "<p>Essa fun√ß√£o serve para retirarmos dentro do nosso array algumas palavras que n√£o s√£o interessantes para contabilizarmos uma pontua√ß√£o na hora de classificar o nosso texto, ent√£o mantemos somente as palavras principais.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWords(sentence):\n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        if word not in stopwords:\n",
    "            phrase.append(word)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Tratando os Dados da Base de Treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tratando com a fun√ß√£o: Removehttp\n",
    "a0 = Remove(group_1)\n",
    "b0 = Remove(group_2)\n",
    "c0 = Remove(group_3)\n",
    "\n",
    "#Tratando com a fun√ß√£o: TratamentoGrupo\n",
    "a1 = TratamentodoGrupo(a0)\n",
    "b1 = TratamentodoGrupo(a0)\n",
    "c1 = TratamentodoGrupo(a0)\n",
    "\n",
    "#Tratando com a fun√ß√£o: Stemming\n",
    "i = 0\n",
    "a2 = len(a1)*[0]\n",
    "while i < len(a1):\n",
    "    a2[i] = Stemming(a1[i])\n",
    "    i+=1\n",
    "    \n",
    "h = 0\n",
    "b2 = len(b1)*[0]\n",
    "while h < len(b1):\n",
    "    b2[h] = Stemming(b1[h])\n",
    "    h+=1\n",
    "\n",
    "j = 0\n",
    "c2 = len(c1)*[0]\n",
    "while j < len(c1):\n",
    "    c2[j] = Stemming(c1[j])\n",
    "    j+=1\n",
    "\n",
    "#Tratando com a fun√ß√£o: RemoveStopWords\n",
    "q = 0\n",
    "a3 = len(a2)*[0]\n",
    "while q < len(a2):\n",
    "    a3[q] = RemoveStopWords(a2[q])\n",
    "    q+=1\n",
    "    \n",
    "w = 0\n",
    "b3 = len(b2)*[0]\n",
    "while w < len(b2):\n",
    "    b3[w] = RemoveStopWords(b2[w])\n",
    "    w+=1\n",
    "\n",
    "l = 0\n",
    "c3 = len(c2)*[0]\n",
    "while l < len(c2):\n",
    "    c3[l] = RemoveStopWords(c2[l])\n",
    "    l+=1\n",
    "\n",
    "groups_tweet = [a3,b3,c3];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Naive Bayes e Laplace Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Fun√ß√£o lista_sem_repeti√ß√£o:\n",
    "<p> A fun√ß√£o abaixo cria uma lista com todas as palavras existentes num grupo sem repeti√ß√£o, adicionando elas na lista total_words. No fim, temos o n√∫mero de palavras sem repeti√ß√£o, a qual √© importante para c√°lculos futuros envolvendo a express√£o de Laplace.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lista_sem_repeticao(LISTA,lista):\n",
    "    resposta = LISTA\n",
    "    for tweet in lista:\n",
    "        for palavra in tweet:\n",
    "            if palavra not in resposta:\n",
    "                resposta.append(palavra)\n",
    "    return resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = []\n",
    "lista_sem_repeticao(total_words,groups_tweet[0])\n",
    "lista_sem_repeticao(total_words,groups_tweet[1])\n",
    "lista_sem_repeticao(total_words,groups_tweet[2])\n",
    "N =len(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Fun√ß√£o laplace:\n",
    "<p> Abaixo, tem-se a fun√ß√£o proposta por Laplace, a qual realiza um smoothing nos dados. Ela recebe tr√™s par√¢metros: \n",
    "</p>\n",
    "<ul>\n",
    "    <li>N√∫mero de vezes que a palavra aparece no grupo;</li>\n",
    "    <li>N√∫mero de elementos contido no grupo (1, 2 ou 3);</li>\n",
    "    <li>N√∫mero de elementos sem repeti√ß√£o em todas os grupos</li>    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace(num_vezes_no_grupo, num_elementos_do_grupo, num_elementos_sem_repeticao):\n",
    "    probabilidade = (num_vezes_no_grupo + 1 ) / ( num_elementos_do_grupo + num_elementos_sem_repeticao)\n",
    "    return probabilidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Fun√ß√£o numero_de_vezes:\n",
    "<p> A fun√ß√£o \"n√∫mero de vezes\" fornece a quantidade de vezes que a palavra aparece no grupo. Essa fun√ß√£o √© utilizada juntamente com Laplace. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numero_de_vezes(palavra_analisada,grupo):\n",
    "    n = 0\n",
    "    for tweet in grupo:\n",
    "        for palavra in tweet:\n",
    "            if palavra ==  palavra_analisada:\n",
    "                n +=1\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []\n",
    "for group in groups_tweet:\n",
    "    dictionary= {}\n",
    "    for tweet in group:\n",
    "        for word in tweet:\n",
    "            probabilidade = laplace(numero_de_vezes(word,group),len(group),N)\n",
    "            dictionary[word] = probabilidade\n",
    "    resultado = pd.Series(dictionary)\n",
    "    resultados.append(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Fun√ß√£o conta_palavras:\n",
    "<p> A fun√ß√£o palavras v√™ quantas existem num grupo, o qual √© composto por um s√©rie de tweets.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conta_palavras(grupo):\n",
    "    n = 0\n",
    "    for tweet in grupo:\n",
    "        n += len(tweet)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_1 = conta_palavras(groups_tweet[0])\n",
    "N_2 = conta_palavras(groups_tweet[1])\n",
    "N_3 = conta_palavras(groups_tweet[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Verificando a performance na Planilha Testes\n",
    "<p>\n",
    "Testando classificador com a base de Testes.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = planilha_teste[\"Tweet Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Fun√ß√£o limpeza_tweet:\n",
    "<p> A fun√ß√£o limpa cada twetet.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpeza_tweet(tweet):\n",
    "    tweet_limpo = SpaceCleaning(str(tweet))\n",
    "    return tweet_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste_tweet = []\n",
    "for tweet in teste:\n",
    "    teste_tweet.append(limpeza_tweet(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie_teste = pd.Series(teste_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Fun√ß√£o analisa_frase:\n",
    "<p> Como os resultados est√£o muito pequenos, usou-se a fun√ß√£o do logar√≠tmo neperiano, ln(x), para os valores ficarem com n√∫meros mais adequados para o Python.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisa_frase(frase):\n",
    "    condicional_grupo1 = 1\n",
    "    condicional_grupo2 = 1\n",
    "    condicional_grupo3 = 1\n",
    "    lista = [condicional_grupo1, condicional_grupo2,condicional_grupo3]\n",
    "    \n",
    "    for palavra in frase:\n",
    "        if (palavra not in resultados[0].index):\n",
    "            condicional_grupo1 *= laplace(0,N_1,N)\n",
    "        else: \n",
    "            condicional_grupo1 *= resultados[0][palavra]\n",
    "            \n",
    "    for palavra in frase:\n",
    "        if (palavra not in resultados[1].index):\n",
    "            condicional_grupo2 *= laplace(0,N_1,N) \n",
    "        else:\n",
    "            condicional_grupo2 *= resultados[1][palavra]\n",
    "            \n",
    "    for palavra in frase:\n",
    "        if (palavra not in resultados[2].index):\n",
    "            condicional_grupo3 *= laplace(0,N_1,N) \n",
    "        else:\n",
    "            condicional_grupo3 *= resultados[2][palavra]\n",
    "            \n",
    "    return [condicional_grupo1,condicional_grupo2,condicional_grupo3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Fun√ß√£o identifica_valor_maximo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifica_valor_maximo(lista_de_resultados):\n",
    "    if lista_de_resultados[0] == max(lista_de_resultados):\n",
    "        return 1\n",
    "    elif lista_de_resultados[1] == max(lista_de_resultados):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Fun√ß√£o resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultado1(n):\n",
    "    return identifica_valor_maximo(analisa_frase(limpeza_tweet(teste[n])))\n",
    "\n",
    "def resultado2(database_coluna, n):\n",
    "    return identifica_valor_maximo(analisa_frase(limpeza_tweet(database_coluna[n])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimativa = []\n",
    "for k in range(0,len(teste)):\n",
    "    estimativa.append(resultado1(k))\n",
    "\n",
    "planilha_teste[\"Relev√¢ncia_Estimada\"] = estimativa\n",
    "\n",
    "Acertos = 0\n",
    "for n in planilha_teste.index:\n",
    "    if planilha_teste[\"Relev√¢ncia_Prevista\"][n] == planilha_teste[\"Relev√¢ncia_Estimada\"][n]:\n",
    "        Acertos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.540000\n",
       "3    0.236667\n",
       "2    0.223333\n",
       "Name: Relev√¢ncia_Prevista, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planilha_teste[\"Relev√¢ncia_Prevista\"].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.492205\n",
       "2.0    0.285078\n",
       "3.0    0.222717\n",
       "Name: Relev√¢ncia, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planilha_treinamento[\"Relev√¢ncia\"].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1.0\n",
       "Name: Relev√¢ncia_Estimada, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planilha_teste[\"Relev√¢ncia_Estimada\"].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Acertos/len(planilha_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tweet(quantidade, autorizacao):\n",
    "    \"\"\"return_tweet(autorizacao, quantidade)\"\"\"\n",
    "    api = tweepy.API(autorizacao)\n",
    "    i = 0\n",
    "    __lista__ = []\n",
    "    anterior = {'User Name': 'foo',\n",
    "                'Tweet Created At': 'foo','Tweet Text': 'foo','Relev√¢ncia': 'foo',\n",
    "                'User Location': 'foo','Phone Type': 'foo',\n",
    "                'Favorite Count': 'foo','Retweets':'foo'}\n",
    "    while i < quantidade:\n",
    "        for msg in tweepy.Cursor(api.search, q=\"{0} -filter:retweets\".format(\"depressao\"), lang='pt', tweet_mode=\"extended\").items():\n",
    "            new_msg = {'Tweet Text': msg.full_text.lower()}\n",
    "            i += 1\n",
    "            if  new_msg[\"Tweet Text\"] not in __lista__:\n",
    "                __lista__.append(new_msg)\n",
    "                if resultado2(list(new_msg[\"Tweet Text\"]),0) == 1:\n",
    "                    __resultado__ = \"Irrelevante\"\n",
    "                elif resultado2(list(new_msg[\"Tweet Text\"]),0) == 2:\n",
    "                    __resultado__ = \"Preocupante\"\n",
    "                elif resultado2(list(new_msg[\"Tweet Text\"]),0) == 3:\n",
    "                    __resultado__ = \"Demonstra interesse pelo tema\"\n",
    "                print(\"[{0}]\\nClassifica√ß√£o:\\t{1}\\nTexto:\\t{2}\".format(i,__resultado__, new_msg[\"Tweet Text\"]))\n",
    "            print(\"\\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\\n\\n\")\n",
    "            sleep(4.5)\n",
    "            if i >= quantidade:\n",
    "                break\n",
    "        print(\"[Fim do Programa]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\teu era t√£o segura de mim mesma que parece que sou outra pessoa depois q me afundei nessa merda dessa depress√£o\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[2]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\tapenas um nerdcast com a agatha e a portuguesa curaria minha depress√£o\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[3]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\tdepress√£o t√° me dando uma surra hoje, n√£o sei o que fiz pra merecer se tomei meus rem√©dios direitinho :c\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[4]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\tuma vez achei q tava com depressao, mas era s√≥ tpm msm https://t.co/yiqqksjuj1\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[5]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\tengra√ßado que as pessoas me olham sorrindo e n√£o imaginam, que eu posso estar em depress√£o\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[6]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\tna moral depress√£o s√≥ aparece pra gente perder tempo\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[7]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\t@patgameiro_ o discurso dele n√£o era sobre depress√£o. eu assisti os stories todos no dia que ele postou (foi bem antes de setembro, inclusive). o tema n√£o foi em momento algum depress√£o. inclusive ele ajuda diversas pessoas a sair da depress√£o. conhe√ßo algumas que melhoraram o acompanhando.\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[8]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\t@creepylittle_ herpes, aids, s√≠filis, avc, perna quebrada, depress√£o, ataque do cora√ß√£o, v√≠cio em crack, a porra toda kskskskssk\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[9]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\thoje refleti sobre os 3 anos da minha vida que sofri as piores crises de depress√£o, ansiedade e p√¢nico\n",
      "de quantas oportunidades eu perdi, mas foi um mal que veio para o bem, mas hj posso esbanjar minha boa sa√∫de mental e progresso\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[10]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\tdepress√£o dinheiro droga e foda\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[11]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\teu n sei se isso √© depress√£o p√≥s show real, mas sei la eu to me sentindo estranha depois do show, eu sinto que eu to a um ponto de chorar, to me sentindo exclu√≠da de novo sla grrr eu n sei o que esta acontecendo\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[12]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\t@depretas oxi, sou capricorniana e s√≥ consigo piorar a depress√£o e ganhar peso. meu ascendente deve ser mto ruim\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[13]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\t@f1eoque_liga quando eu vejo a foto de vcs dois, eu vejo o sorriso dela e me lembro de como ela era feliz antes da depress√£o se manifestar, e de como ela era linda e n√£o d√° pra acreditar q ela se foi, e q agora oq resta √© sdd e lembran√ßas de uma menina incrivel‚ù§üòî\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[14]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\t@flaviooliveiros vixe, vc concorda at√© no discurso dele sobre depress√£o?\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[15]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\to governo lan√ßando campanha contra depress√£o nos jovens sendo que os jovens n√£o tem um dia de paz no atual governo üëç https://t.co/nna7a8ozx0\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[16]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\t@brunaalvarenga0 hallo the unroll you asked for: thread by @pablovillaca: \"nos √∫ltimos dois dias, a depress√£o, esta velha inimiga fiel, tem me atormentado com uma for√ßa mais condizente com a de u [‚Ä¶]\" https://t.co/vfd8xxyxkz\n",
      "enjoy :) ü§ñ\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[17]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\tque depress√£o ouvir esse √°lbum viado\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[18]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\teu me pego diversas vezes n√£o sendo anti racista e fazendo o mesmo que gente que nunca teve depress√£o quando tenta ajudar quem tem. isso √© horr√≠vel\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[19]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\tqueria o diva depress√£o na fazenda\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[20]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\teu t√¥ observando algumas pessoas romantizando depress√£o e ansiedade, como se fosse a coisa mais bacana e descolada... poxa, tenho p√©ssimas not√≠cias em!\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[21]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\tvoc√™ que paga de entendida a depress√£o,que quando um amigo se mata,fica a√≠ se lamentando pelos cantos.. para,pq enquanto tem gente gritando,voc√™ t√° rindo com uns ou outros da ‚Äúpalha√ßada‚Äù do amiguinho. se poupe\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[22]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\tminha m√£e me contou que meu tio estava preocupadi que minha prima estava com depress√£o (ela tem 11 anos)\n",
      "eu vi uns shorts dela sobre suic√≠dio..  eu entendi q as redes sociais estavam afetando mt ela, eu, quem nem sou pr√≥xima notei... cont.\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[23]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\t@tantotupiassu @beatzri e menos estigmas.\n",
      "\n",
      "na gera√ß√£o passada de minha fam√≠lia h√° td sorte de doen√ßas mentais (bipolaridade, esquizofrenia, alzeheimer, parkinson, depress√£o), mas nada se falava: era tudo ocultado por vergonha. hoje, incrivelmte, parecemos mais saud√°veis pq falamos e tentamos melhorar.\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[24]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\trecentemente descobriram que ela t√° com depress√£o e anda tomando rem√©dio e fazendo terapia\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[25]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\t-voc√™ j√° teve depress√£o? \n",
      "-sim \n",
      "-e j√° teve que ouvir algu√©m falando que era s√≥ drama?\n",
      "-n√£o \n",
      "-e j√° teve depress√£o???\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n",
      "[26]\n",
      "Classifica√ß√£o:\tIrrelevante\n",
      "Texto:\t@bxlweldss a p√°gina √© a etim da depress√£o\n",
      "\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-aec95e59c046>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreturn_tweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# return_tweet(quantidade, autentica√ß√£o)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-8336f1b68dc1>\u001b[0m in \u001b[0;36mreturn_tweet\u001b[1;34m(quantidade, autorizacao)\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[{0}]\\nClassifica√ß√£o:\\t{1}\\nTexto:\\t{2}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m__resultado__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_msg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Tweet Text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t -=-=-=-=-= aguardando 4.5 segundos =-=-=-=-=-\\n\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mquantidade\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "return_tweet(50,auth) # return_tweet(quantidade, autentica√ß√£o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclus√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Diante do que foi observado, pode-se verificar que a base de dados forneceu um acerto em torno de 54%. Ou seja, mesmo que o resultado n√£o seja t√£o expressivo, o verificador pode ser √∫til na identifica√ß√£o de pessoas que possuem indicativo de depress√£o. Nesse contexto, pode-se pensar em melhorias na base de dados, afim de otimizar o acerto. Um fator que foi utilizado para melhorar o tratamentos dos tweets foi o stemming, que transforma palavras de mesmo radical, bem como a exclus√£o de preposi√ß√µes e conectivos, que n√£o auxiliam no julgamento do tweet. An√°logo a essa melhoria, pode-se pensar em outras maneiras de tratar as palavras, como substituir palavras que s√£o sin√¥nimos.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)</li>\n",
    "\n",
    "<li>[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/)</li>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
